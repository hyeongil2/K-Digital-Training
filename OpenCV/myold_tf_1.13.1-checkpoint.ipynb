{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install opencv-python\n",
    "# ! pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# from tensorflow.python.framework import graph_util\n",
    "# from tensorflow.python.platform import gfile\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# mnist = input_data.read_data_sets('./MNIST_data/', one_hot=True)\n",
    "\n",
    "# #\n",
    "# # hyper parameters\n",
    "# #\n",
    "# learning_rate = 0.001\n",
    "# training_epochs = 20\n",
    "# batch_size = 100\n",
    "\n",
    "# #\n",
    "# # Model configuration\n",
    "# #\n",
    "# X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='data')\n",
    "# Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# conv1 = tf.layers.conv2d(X, 10, [3, 3], padding='same', activation=tf.nn.relu)\n",
    "# pool1 = tf.layers.max_pooling2d(conv1, [2, 2], strides=2, padding='same')\n",
    "\n",
    "# conv2 = tf.layers.conv2d(pool1, 20, [3, 3], padding='same', activation=tf.nn.relu)\n",
    "# pool2 = tf.layers.max_pooling2d(conv2, [2, 2], strides=2, padding='same')\n",
    "\n",
    "# fc1 = tf.contrib.layers.flatten(pool2)\n",
    "# fc2 = tf.layers.dense(fc1, 200, activation=tf.nn.relu)\n",
    "# logits = tf.layers.dense(fc2, 10, activation=None)\n",
    "# output = tf.nn.softmax(logits, name='prob')\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# #\n",
    "# # Training\n",
    "# #\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# print('Start learning!')\n",
    "# for epoch in range(training_epochs):\n",
    "#     total_cost = 0\n",
    "\n",
    "#     for i in range(total_batch):\n",
    "#         batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "#         batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "#         _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "#         total_cost += cost_val\n",
    "\n",
    "#     print('Epoch: {0}, Avg. Cost = {1:.4f}'.format(epoch + 1, total_cost/total_batch))\n",
    "\n",
    "# print('Learning finished!')\n",
    "\n",
    "# # Test the results\n",
    "# is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "# acc = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "# accuracy = sess.run(acc, feed_dict={\n",
    "#                     X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "# print('Test Accuracy:', accuracy)\n",
    "\n",
    "# # Freeze variables and save pb file\n",
    "# output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['prob'])\n",
    "# with gfile.FastGFile('./mnist_cnn.pb', 'wb') as f:\n",
    "#     f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "# print('mnist_cnn.pb file is created successfully!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning을 이용한 필기체 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.dnn.readNet(model, config=None, framework=None) -> retval\n",
    "    # model:훈련된 가중치 파일\n",
    "    # config: 네트워크 구성을 저장하고 있는 텍스트 파일 이름\n",
    "    # framework: None\n",
    "    # retval: cv2.dnn_Net 클래스 객체\n",
    "    \n",
    "# cv2.dnn.blobFromImage(image, scalefactor=None, size=None, mean=None,\n",
    "#                         swapRB=None, crop=None, ddepth=None)\n",
    "    # image: 입력영상\n",
    "    # scalefactor: 입력영상 픽셀에 곱할 값, 기본값은 1\n",
    "    # size: 츨력영상의 크기, 기본값은 (0,0)\n",
    "    # mean: 입력영상 각 채널에서 뺄 평균값\n",
    "    # swapRB: R,B 채널의 순서, True or False\n",
    "    # ddepth: 출력영상의 깊이 CV_32F\n",
    "    # retval:blob객체, numpy.ndarray, shape=(N,C,H,W), dtype=numpy.float32\n",
    "\n",
    "# net.setInput(blob, name=None, scalefactor=None, mean=None)-> None\n",
    "    # blob: blob객체\n",
    "    # name\n",
    "    # scalefactor\n",
    "    # mean\n",
    "\n",
    "# net.forward(outputName=None) -> retval\n",
    "    # outputName: 출력레이어 이름\n",
    "    # retval: dnn 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# cv2에서는 러닝은 안됨. 훈련 완료한 모델을 이용하여 딥러닝을 사용\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "oldx, oldy = -1, -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        oldx, oldy = -1, -1\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), (255, 255, 255), 40, cv2.LINE_AA)\n",
    "            oldx, oldy = x, y\n",
    "            cv2.imshow('img', img)\n",
    "\n",
    "net = cv2.dnn.readNet('mnist_cnn.pb') # 러닝한 파라미터 불러들이기\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed!')\n",
    "    sys.exit() # 잘 불럿는지 확인\n",
    "\n",
    "img = np.zeros((400, 400), np.uint8) #400*400 크기에다가 \n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.setMouseCallback('img', on_mouse) # 마우스 콜백\n",
    "\n",
    "while True:\n",
    "    c = cv2.waitKey()\n",
    "\n",
    "    if c == 27:\n",
    "        break\n",
    "        \n",
    "    elif c == ord(' '): # space bar 누르면 실행\n",
    "        blob = cv2.dnn.blobFromImage(img, 1/255., (28, 28)) # (28, 28) 영상의 사이즈를 써서 mneast자료 크기\n",
    "        net.setInput(blob)\n",
    "        prob = net.forward() # prob로 반환 -> \n",
    "\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob) # 확률이 가장 큰 값을 찾는다. (softmax)\n",
    "        digit = maxLoc[0] \n",
    "\n",
    "        print(f'{digit} ({maxVal * 100:4.2f}%)')\n",
    "\n",
    "        img.fill(0)\n",
    "        cv2.imshow('img', img)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 2를 입력했는데 99.99% 확률로 3이라고 인식... ㅎㅎㅋㅋㅋ\n",
    "# 위치 조정을 통해 spatial transform 필요할듯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/463748de-26e9-4db4-b19c-1df072cf024d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T054444Z&X-Amz-Expires=86400&X-Amz-Signature=f8d190edb09fdd3c302887789a55e7896d867977574cfd5563d5153c1e742630&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleNet 영상인식 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://modelzoo.co/model/googlenet-v2 모델파일 다운로드\n",
    "\n",
    "# Caffe Model Zoo : github.com/BVLC/caffe\n",
    "## 모델 파일 : dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel\n",
    "## 설정 파일 : github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt\n",
    "# -> 네트워크 정보\n",
    "\n",
    "# ONNX Model Zoo : github.com/onnx/models \n",
    "\n",
    "# 클래스 이름 파일 : github.com/opencv/opencv/blob/4.1.0/samples/data/dnn/classification_classes_ILSVRC2012.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- googlenet 구조 \n",
    "- https://poddeeplearning.readthedocs.io/ko/latest/CNN/GoogLeNet/\n",
    "\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/fbc984cb-5625-41d6-bd51-bc10a0f37051/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T062459Z&X-Amz-Expires=86400&X-Amz-Signature=44c4e110dbf9049eb4fb3dcad00e4813f178787bba83df4333290b5ec8222460&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 입력 영상 불러오기\n",
    "\n",
    "# filename = 'apple1.png'\n",
    "filename = './googlenet/pineapple.jpg'\n",
    "\n",
    "# if len(sys.argv) > 1: \n",
    "#     filename = sys.argv[1]\n",
    "\n",
    "img = cv2.imread(filename)\n",
    "\n",
    "if img is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 네트워크 불러오기\n",
    "\n",
    "# 네트워크1 - Caffe\n",
    "model = 'googlenet/bvlc_googlenet.caffemodel'\n",
    "config = 'googlenet/deploy.prototxt.txt'\n",
    "\n",
    "# 네트워크2 - ONNX\n",
    "# model = 'googlenet/googlenet-9.onnx'\n",
    "# config = ''\n",
    "# -> config 없어도 됨\n",
    "\n",
    "# network 객체 만들기\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 클래스 이름 불러오기\n",
    "classNames = []\n",
    "with open('googlenet/classification_classes_ILSVRC2012.txt', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "# 추론\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (224, 224), (104, 117, 123))\n",
    "# 1 - 0~255까지 스케일 팩터를 사용하겠다\n",
    "# (224, 224) - 이미지 디멘션 (우리가 넣어줄 영상)\n",
    "# (104, 117, 123) - 칼라 (mean substraction)\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "\n",
    "# 추론 결과 확인 & 화면 출력\n",
    "out = prob.flatten()\n",
    "classId = np.argmax(out)\n",
    "confidence = out[classId]\n",
    "\n",
    "text = f'{classNames[classId]} ({confidence * 100:4.2f}%)'\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a33f640e-3bdc-4e43-9cad-3be196d515ae/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T064448Z&X-Amz-Expires=86400&X-Amz-Signature=e0f89f98aa4467a74b01498e58928961657e2b1abe0dce533e3cb01df3132317&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV DNN 얼굴검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\n",
    "\n",
    "# deploy.prototxt.txt, download-weights.py.txt, opencv_face_detector.pbtxt.text 다운로드\n",
    "\n",
    "# Caffe    https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "\n",
    "# Tensorflow  https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180220_uint8/opencv_face_detector_uint8.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Caffe\n",
    "model = 'opencv_face_detector/opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "config = 'opencv_face_detector/opencv_face_detector/deploy.prototxt'\n",
    "\n",
    "# model = 'opencv_face_detector/opencv_face_detector/opencv_face_detector_uint8.pb'\n",
    "# config = 'opencv_face_detector/opencv_face_detector/opencv_face_detector.pbtxt'\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Camera open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "net = cv2.dnn.readNet(model, config) # 객체 만들기\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123))\n",
    "    net.setInput(blob)\n",
    "    out = net.forward() # out.shape = (1, 1, 200, 7) -> 200*7만 사용\n",
    "\n",
    "    detect = out[0, 0, :, :] # 0, 0, 사용안함, # 3번째~7 컬럼부터 사용 ( , , confidence, x1, x2, y1, y2) -> x 상단, y 하단\n",
    "    (h, w) = frame.shape[:2] \n",
    "\n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i, 2]\n",
    "        if confidence < 0.5:\n",
    "            break\n",
    "\n",
    "        x1 = int(detect[i, 3] * w)\n",
    "        y1 = int(detect[i, 4] * h)\n",
    "        x2 = int(detect[i, 5] * w)\n",
    "        y2 = int(detect[i, 6] * h)\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0))\n",
    "\n",
    "        label = f'Face: {confidence:4.2f}'\n",
    "        cv2.putText(frame, label, (x1, y1 - 1), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 얼굴인식 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/244cf78a-cf23-4e9d-a844-dcc7874737fc/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T072943Z&X-Amz-Expires=86400&X-Amz-Signature=5fe624f4189402ed8831d330a559e4c3e7f82b96be4fae629c691036a1be9ad1&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv3를 이용한 객체 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pjreddie.com/darknet/yolo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 모델 & 설정 파일\n",
    "model = 'yolo_v3/yolo_v3/yolov3.weights' # parameter 정보\n",
    "config = 'yolo_v3/yolo_v3/yolov3.cfg'\n",
    "class_labels = 'yolo_v3/yolo_v3/coco.names'\n",
    "confThreshold = 0.5\n",
    "nmsThreshold = 0.4\n",
    "\n",
    "# 테스트 이미지 파일\n",
    "img_files = ['yolo_v3/yolo_v3/dog.jpg', 'yolo_v3/yolo_v3/person.jpg', \n",
    "             'yolo_v3/yolo_v3/sheep.jpg', 'yolo_v3/yolo_v3/kite.jpg']\n",
    "\n",
    "# 네트워크 생성\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 클래스 이름 불러오기\n",
    "\n",
    "classes = []\n",
    "with open(class_labels, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# 출력 레이어 이름 받아오기\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "# output_layers = ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "# 이 네트웤에서 82, 94, 106에서 결과를 뽑아라\n",
    "\n",
    "# 실행\n",
    "for f in img_files:\n",
    "    img = cv2.imread(f)\n",
    "\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    # 블롭 생성 & 추론\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255., (320, 320), swapRB=True) \n",
    "    # 0~1스케일로 만들고, 320*320 사이즈, swapRB=TrueRGB로 하겠다.\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers) #\n",
    "\n",
    "    # outs는 3개의 ndarray 리스트.\n",
    "    # outs[0].shape=(507, 85), 13*13*3=507\n",
    "    # outs[1].shape=(2028, 85), 26*26*3=2028\n",
    "    # outs[2].shape=(8112, 85), 52*52*3=8112\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            # detection: 4(bounding box) + 1(objectness_score) + 80(class confidence)\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confThreshold:\n",
    "                # 바운딩 박스 중심 좌표 & 박스 크기\n",
    "                cx = int(detection[0] * w)\n",
    "                cy = int(detection[1] * h)\n",
    "                bw = int(detection[2] * w)\n",
    "                bh = int(detection[3] * h)\n",
    "\n",
    "                # 바운딩 박스 좌상단 좌표\n",
    "                sx = int(cx - bw / 2)\n",
    "                sy = int(cy - bh / 2)\n",
    "\n",
    "                boxes.append([sx, sy, bw, bh])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(int(class_id))\n",
    "\n",
    "    # 비최대 억제, Non Max Suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        sx, sy, bw, bh = boxes[i]\n",
    "        label = f'{classes[class_ids[i]]}: {confidences[i]:.2}'\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(img, (sx, sy, bw, bh), color, 2)\n",
    "        cv2.putText(img, label, (sx, sy - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "    cv2.putText(img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/156a7a55-4f8f-4a32-8e9d-6f2c64539109/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T073659Z&X-Amz-Expires=86400&X-Amz-Signature=748a8af1e26c4514f0fa647bec3a1ff16762bdd014209df98bf02db36712a622&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4a522319-72b8-45e8-9bab-83de49f29b6a/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T073731Z&X-Amz-Expires=86400&X-Amz-Signature=d20ef98b1cd0ac9589bba328ccd122fde087c7898f8398558ca5f0f69394b678&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/7848c7d8-e769-45ad-8daf-18233df129ca/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T073757Z&X-Amz-Expires=86400&X-Amz-Signature=cc3acc9eb643eb6b43a041a6d70e1940c6e9a1d72c2dcfa3dbd3f13ddee3905a&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/7dfc2f73-47b6-4790-9127-784c19967724/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210709%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210709T073828Z&X-Amz-Expires=86400&X-Amz-Signature=eda140980b16579ec57cb5af0d0e6477bea9dea9899a96b58ccc73d8b98a3f97&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
